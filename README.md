# llm-finetuning-and-quantization
Implementation of model optimization techniques including baseline fine-tuning, post-training quantization (PTQ), quantization-aware training (QAT), and Quantized Low-Rank Adaptation (QLoRA) for efficient large language model training and deployment.
