{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohitptnk/llm-finetuning-and-quantization/blob/main/Colab_File.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ----- ALL MODEL EVALUATION -----"
      ],
      "metadata": {
        "id": "1xfDYgsSUg_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets evaluate scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkJ5yN_-WOxp",
        "outputId": "57991afa-1776-4997-965e-ce68a91eca32"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QLORA"
      ],
      "metadata": {
        "id": "BxxprxgqjpdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U \"bitsandbytes\" \"peft\" \"accelerate\" \"evaluate\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gm_mhx-lGoI",
        "outputId": "d3ac4227-afad-45d3-d386-2b1cd20cfbd4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m556.4/556.4 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rohitptnk/llm-finetuning-and-quantization.git\n",
        "!cd llm-finetuning-and-quantization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldLsS9jYQ9XE",
        "outputId": "30c6c927-03f2-4b32-d272-b316e2217036"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llm-finetuning-and-quantization'...\n",
            "remote: Enumerating objects: 44, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 44 (delta 8), reused 31 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (44/44), 325.64 KiB | 21.71 MiB/s, done.\n",
            "Resolving deltas: 100% (8/8), done.\n",
            "Filtering content: 100% (4/4), 1011.02 MiB | 12.72 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qlora_path = \"/content/llm-finetuning-and-quantization/QLORA\"\n",
        "\n",
        "from transformers import BertForSequenceClassification, BertTokenizerFast\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Loading model\n",
        "from peft import PeftModel\n",
        "dataset = load_dataset(\"dair-ai/emotion\", \"split\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "num_labels = len(set(dataset[\"train\"][\"label\"]))\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=num_labels,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "\n",
        "qlora_model = PeftModel.from_pretrained(base_model, qlora_path)\n",
        "qlora_model.eval()\n",
        "\n",
        "# Loading Data\n",
        "ds = load_dataset(\"dair-ai/emotion\", \"split\")\n",
        "MAX_LEN = 256\n",
        "\n",
        "def encode(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=MAX_LEN\n",
        "    )\n",
        "\n",
        "encoded = ds.map(encode, batched=True)\n",
        "encoded.set_format(\"torch\", columns=[\"input_ids\",\"attention_mask\",\"label\"])\n",
        "\n",
        "# Running Evaluation\n",
        "test_loader = DataLoader(encoded[\"test\"], batch_size=32)\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "qlora_model.eval()\n",
        "\n",
        "for batch in test_loader:\n",
        "    input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
        "    attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
        "    labels = batch[\"label\"].numpy()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = qlora_model(input_ids=input_ids,\n",
        "                       attention_mask=attention_mask).logits\n",
        "        preds = logits.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "    all_preds.extend(preds)\n",
        "    all_labels.extend(labels)\n",
        "\n",
        "# ----- Metrics -----\n",
        "print(\"============================= QLORA Report =============================\")\n",
        "# Accuaracy\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "print(\"Accuracy:\", acc)\n",
        "\n",
        "# Macro F1 + Per-class F1\n",
        "macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "print(\"Macro F1:\", macro_f1)\n",
        "print(\"\\nPer-class F1:\")\n",
        "print(classification_report(all_labels, all_preds, digits=4))\n",
        "\n",
        "#Confusion Matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(all_labels, all_preds))\n",
        "\n",
        "# Model Size\n",
        "import os\n",
        "model_path = qlora_path\n",
        "size_mb = os.path.getsize(\"/content/llm-finetuning-and-quantization/QLORA/adapter_model.safetensors\") / 1e6\n",
        "print(\"Model size (MB):\", size_mb)\n",
        "\n",
        "# Latency\n",
        "import time\n",
        "example = encoded[\"test\"][0]\n",
        "inputs = {\n",
        "    \"input_ids\": example[\"input_ids\"].unsqueeze(0).to(\"cuda\"),\n",
        "    \"attention_mask\": example[\"attention_mask\"].unsqueeze(0).to(\"cuda\")\n",
        "}\n",
        "N = 100\n",
        "start = time.time()\n",
        "with torch.no_grad():\n",
        "    for _ in range(N):\n",
        "        _ = qlora_model(**inputs)\n",
        "end = time.time()\n",
        "lat_ms = (end - start) / N * 1000\n",
        "print(\"Latency (ms/example):\", lat_ms)"
      ],
      "metadata": {
        "id": "qhpbgOmAfJ1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full Finetune Report"
      ],
      "metadata": {
        "id": "Qxp3hmYReyjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finetune_path = \"/content/llm-finetuning-and-quantization/full-finetune\"\n",
        "\n",
        "from transformers import BertForSequenceClassification, BertTokenizerFast\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "# Loading model\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained(finetune_path)\n",
        "model = BertForSequenceClassification.from_pretrained(finetune_path)\n",
        "model.eval().to(\"cuda\")\n",
        "\n",
        "# Loading Data\n",
        "ds = load_dataset(\"dair-ai/emotion\", \"split\")\n",
        "MAX_LEN = 256\n",
        "\n",
        "def encode(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=MAX_LEN\n",
        "    )\n",
        "\n",
        "encoded = ds.map(encode, batched=True)\n",
        "encoded.set_format(\"torch\", columns=[\"input_ids\",\"attention_mask\",\"label\"])\n",
        "\n",
        "# Running Evaluation\n",
        "test_loader = DataLoader(encoded[\"test\"], batch_size=32)\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "model.eval()\n",
        "\n",
        "for batch in test_loader:\n",
        "    input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
        "    attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
        "    labels = batch[\"label\"].numpy()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids=input_ids,\n",
        "                       attention_mask=attention_mask).logits\n",
        "        preds = logits.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "    all_preds.extend(preds)\n",
        "    all_labels.extend(labels)\n",
        "\n",
        "# ----- Metrics -----\n",
        "print(\"============================= Full Finetune Report =============================\")\n",
        "# Accuaracy\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "print(\"Accuracy:\", acc)\n",
        "\n",
        "# Macro F1 + Per-class F1\n",
        "macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "print(\"Macro F1:\", macro_f1)\n",
        "print(\"\\nPer-class F1:\")\n",
        "print(classification_report(all_labels, all_preds, digits=4))\n",
        "\n",
        "#Confusion Matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(all_labels, all_preds))\n",
        "\n",
        "# Model Size\n",
        "import os\n",
        "size_mb = os.path.getsize(\"/content/llm-finetuning-and-quantization/full-finetune/model.safetensors\") / 1e6\n",
        "print(\"Model size (MB):\", size_mb)\n",
        "\n",
        "# Latency\n",
        "import time\n",
        "example = encoded[\"test\"][0]\n",
        "inputs = {\n",
        "    \"input_ids\": example[\"input_ids\"].unsqueeze(0).to(\"cuda\"),\n",
        "    \"attention_mask\": example[\"attention_mask\"].unsqueeze(0).to(\"cuda\")\n",
        "}\n",
        "N = 100\n",
        "start = time.time()\n",
        "with torch.no_grad():\n",
        "    for _ in range(N):\n",
        "        _ = model(**inputs)\n",
        "end = time.time()\n",
        "lat_ms = (end - start) / N * 1000\n",
        "print(\"Latency (ms/example):\", lat_ms)"
      ],
      "metadata": {
        "id": "DY_vRcZOHVjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PTQ Report"
      ],
      "metadata": {
        "id": "mA_dONjFYsqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ptq_path = \"/content/llm-finetuning-and-quantization/PTQ/bert_ptq.pth\"\n",
        "\n",
        "from transformers import BertForSequenceClassification, BertTokenizerFast\n",
        "import torch\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained(finetune_path)\n",
        "model_fp32 = BertForSequenceClassification.from_pretrained(finetune_path)\n",
        "\n",
        "import torch.quantization as tq\n",
        "model_int8 = tq.quantize_dynamic(\n",
        "    model_fp32,\n",
        "    {torch.nn.Linear},\n",
        "    dtype=torch.qint8\n",
        ")\n",
        "\n",
        "state_dict = torch.load(ptq_path, map_location=\"cpu\")\n",
        "model_int8.load_state_dict(state_dict)\n",
        "model_int8.eval()\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, confusion_matrix, classification_report\n",
        ")\n",
        "import numpy as np\n",
        "ds = load_dataset(\"dair-ai/emotion\", \"split\")\n",
        "test_texts = ds[\"test\"][\"text\"]\n",
        "test_labels = ds[\"test\"][\"label\"]\n",
        "preds = []\n",
        "for text in test_texts:\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
        "    with torch.no_grad():\n",
        "        logits = model_int8(**inputs).logits\n",
        "        preds.append(logits.argmax(dim=-1).item())\n",
        "\n",
        "print(\"========================= PTQ REPORT ==================================\")\n",
        "print(\"Accuracy:\", accuracy_score(test_labels, preds))\n",
        "print(\"Macro F1:\", f1_score(test_labels, preds, average=\"macro\"))\n",
        "print(\"\\nPer-class F1:\\n\", classification_report(test_labels, preds))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(test_labels, preds))\n",
        "\n",
        "import os\n",
        "size_mb = os.path.getsize(ptq_path) / 1e6\n",
        "print(\"PTQ model size (MB):\", size_mb)\n",
        "\n",
        "import time\n",
        "example = encoded[\"test\"][0]\n",
        "inputs = {\n",
        "    \"input_ids\": example[\"input_ids\"].unsqueeze(0).to(\"cpu\"),\n",
        "    \"attention_mask\": example[\"attention_mask\"].unsqueeze(0).to(\"cpu\")\n",
        "}\n",
        "N = 100\n",
        "start = time.time()\n",
        "with torch.no_grad():\n",
        "    for _ in range(N):\n",
        "        _ = model_int8(**inputs)\n",
        "end = time.time()\n",
        "print(\"PTQ latency (ms/example):\", (end - start)/N * 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2l2mvq1XV22",
        "outputId": "6cc948d4-f7c9-46c9-c2f2-81be90780f9e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2117623386.py:10: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_int8 = tq.quantize_dynamic(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_utils.py:444: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  device=storage.device,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================= PTQ REPORT ==================================\n",
            "Accuracy: 0.926\n",
            "Macro F1: 0.8733927330584225\n",
            "\n",
            "Per-class F1:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96       581\n",
            "           1       0.91      0.98      0.94       695\n",
            "           2       0.90      0.72      0.80       159\n",
            "           3       0.98      0.88      0.93       275\n",
            "           4       0.87      0.95      0.91       224\n",
            "           5       0.93      0.56      0.70        66\n",
            "\n",
            "    accuracy                           0.93      2000\n",
            "   macro avg       0.92      0.84      0.87      2000\n",
            "weighted avg       0.93      0.93      0.92      2000\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[566   9   0   1   5   0]\n",
            " [  2 680  12   0   0   1]\n",
            " [  0  45 114   0   0   0]\n",
            " [ 18   5   0 243   9   0]\n",
            " [  6   0   0   4 212   2]\n",
            " [  3   9   0   0  17  37]]\n",
            "PTQ model size (MB): 181.497448\n",
            "PTQ latency (ms/example): 395.4185175895691\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QAT Report"
      ],
      "metadata": {
        "id": "5wPR0Vz9erTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qat_model_path = \"/content/llm-finetuning-and-quantization/QAT\"\n",
        "\n",
        "from transformers import BertForSequenceClassification, BertTokenizerFast\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, confusion_matrix, classification_report\n",
        ")\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "qat_model = BertForSequenceClassification.from_pretrained(qat_model_path)\n",
        "tokenizer = BertTokenizerFast.from_pretrained(qat_model_path)\n",
        "qat_model.eval()\n",
        "\n",
        "import torch.quantization as tq\n",
        "\n",
        "model_int8 = tq.quantize_dynamic(\n",
        "    qat_model,\n",
        "    {torch.nn.Linear},       # quantize only Linear layers\n",
        "    dtype=torch.qint8\n",
        ")\n",
        "\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "model_int8.to(device)\n",
        "model_int8.eval()\n",
        "\n",
        "# Loading Data\n",
        "ds = load_dataset(\"dair-ai/emotion\", \"split\")\n",
        "MAX_LEN = 128\n",
        "\n",
        "def encode(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=MAX_LEN\n",
        "    )\n",
        "\n",
        "encoded = ds.map(encode, batched=True)\n",
        "encoded.set_format(\"torch\", columns=[\"input_ids\",\"attention_mask\",\"label\"])\n",
        "encoded[\"test\"] = encoded[\"test\"].select(range(100))\n",
        "\n",
        "# Running Evaluation\n",
        "test_loader = DataLoader(encoded[\"test\"], batch_size=32)\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "for batch in test_loader:\n",
        "    input_ids = batch[\"input_ids\"].to(device)\n",
        "    attention_mask = batch[\"attention_mask\"].to(device)\n",
        "    labels = batch[\"label\"].numpy()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model_int8(input_ids=input_ids,\n",
        "                       attention_mask=attention_mask).logits\n",
        "        preds = logits.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "    all_preds.extend(preds)\n",
        "    all_labels.extend(labels)\n",
        "\n",
        "# ----- Metrics -----\n",
        "print(\"============================= QAT Report ============================\")\n",
        "# Accuaracy\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "print(\"Accuracy:\", acc)\n",
        "\n",
        "# Macro F1 + Per-class F1\n",
        "macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "print(\"Macro F1:\", macro_f1)\n",
        "print(\"\\nPer-class F1:\")\n",
        "print(classification_report(all_labels, all_preds, digits=4))\n",
        "\n",
        "#Confusion Matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(all_labels, all_preds))\n",
        "\n",
        "# Model Size\n",
        "import os\n",
        "# Path to the saved QAT model directory\n",
        "\n",
        "size_mb = sum(os.path.getsize(os.path.join(qat_model_path, f))\n",
        "              for f in os.listdir(qat_model_path)) / 1e6\n",
        "print(\"Model size (MB):\", size_mb)\n",
        "\n",
        "# Latency\n",
        "import time\n",
        "example = encoded[\"test\"][0]\n",
        "inputs = {\n",
        "    \"input_ids\": example[\"input_ids\"].unsqueeze(0).to(device),\n",
        "    \"attention_mask\": example[\"attention_mask\"].unsqueeze(0).to(device)\n",
        "}\n",
        "N = 100\n",
        "start = time.time()\n",
        "with torch.no_grad():\n",
        "    for _ in range(N):\n",
        "        _ = model_int8(**inputs)\n",
        "end = time.time()\n",
        "lat_ms = (end - start) / N * 1000\n",
        "print(\"Latency (ms/example):\", lat_ms)\n"
      ],
      "metadata": {
        "id": "GOoXOktpYBJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IV-Tpq_Bj4E-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Code"
      ],
      "metadata": {
        "id": "GS_Ymhx2yYlg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z0Zg_ThYsXk"
      },
      "source": [
        "## 1. Baseline fine-tuning (FP32/FP16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOOiRS542vMH"
      },
      "outputs": [],
      "source": [
        "# %pip install torch transformers datasets evaluate -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1C2G0vmLYsIk"
      },
      "outputs": [],
      "source": [
        "# from datasets import load_dataset\n",
        "\n",
        "# ds = load_dataset(\"dair-ai/emotion\", \"split\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCY_F-EIYJRV"
      },
      "outputs": [],
      "source": [
        "# ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7z8LCE9Qcuyw"
      },
      "outputs": [],
      "source": [
        "# ds['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Re2Rq67ic-wP"
      },
      "outputs": [],
      "source": [
        "# from transformers import BertTokenizerFast, BertForSequenceClassification, TrainingArguments, Trainer\n",
        "# import evaluate\n",
        "# import numpy as np\n",
        "\n",
        "# tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# # Tokenize\n",
        "# def tokenize_function(examples):\n",
        "#     return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
        "\n",
        "# tokenized_ds = ds.map(tokenize_function, batched=True)\n",
        "\n",
        "# tokenized_ds = tokenized_ds.rename_column(\"label\", \"labels\")\n",
        "# # Convert to PyTorch\n",
        "# tokenized_ds.set_format(\"torch\")\n",
        "\n",
        "# num_labels = len(set(ds['train']['label']))\n",
        "# # model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSGKRFTv5fNo"
      },
      "outputs": [],
      "source": [
        "# import transformers\n",
        "# print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znZfxOl4zsp6"
      },
      "outputs": [],
      "source": [
        "# accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "# def compute_metrics(p):\n",
        "#   preds = np.argmax(p.predictions, axis=1)\n",
        "#   return accuracy.compute(predictions=preds, references=p.label_ids)\n",
        "\n",
        "# # training_args = TrainingArguments(\n",
        "# #     output_dir=\"/content/drive/MyDrive/Colab Notebooks/NLP Asg 2/bert-emotion\",\n",
        "# #     report_to=\"none\",\n",
        "# #     eval_strategy=\"epoch\",\n",
        "# #     save_strategy=\"epoch\",\n",
        "# #     learning_rate=2e-5,\n",
        "# #     per_device_train_batch_size=16,\n",
        "# #     per_device_eval_batch_size=16,\n",
        "# #     num_train_epochs=3,\n",
        "# #     weight_decay=0.01,\n",
        "# #     load_best_model_at_end=True,\n",
        "# #     logging_dir=\"/content/drive/MyDrive/Colab Notebooks/NLP Asg 2/logs\"\n",
        "# # )\n",
        "\n",
        "# # trainer = Trainer(\n",
        "# #     model=model,\n",
        "# #     args=training_args,\n",
        "# #     train_dataset=tokenized_ds[\"train\"],\n",
        "# #     eval_dataset=tokenized_ds[\"validation\"],\n",
        "# #     tokenizer=tokenizer,\n",
        "# #     compute_metrics=compute_metrics,\n",
        "# # )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89fSBuQl0deW"
      },
      "outputs": [],
      "source": [
        "# trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjLUl_Gx0f34"
      },
      "outputs": [],
      "source": [
        "# trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2UFiGhh0hut"
      },
      "outputs": [],
      "source": [
        "# trainer.save_model(\"/content/drive/MyDrive/Colab Notebooks/NLP Asg 2/bert-emotion-finetuned\")\n",
        "# tokenizer.save_pretrained(\"/content/drive/MyDrive/Colab Notebooks/NLP Asg 2/bert-emotion-finetuned\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogcJFGDi0tU5"
      },
      "outputs": [],
      "source": [
        "# preds = trainer.predict(tokenized_ds[\"test\"])\n",
        "# print(preds.metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcvQmsYSWAAv"
      },
      "source": [
        "## 2. Quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plXfeWl1vD-7"
      },
      "source": [
        "Loading Previous Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1tOtmcX6pWy"
      },
      "outputs": [],
      "source": [
        "# from transformers import BertForSequenceClassification, BertTokenizer\n",
        "\n",
        "# model = BertForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/NLP Asg 2/bert-emotion-finetuned\")\n",
        "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "# model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qjpxnp7EvKBQ"
      },
      "source": [
        "Preparing for PTQ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTdIFnF_vMb_"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# from torch.quantization import quantize_dynamic\n",
        "\n",
        "# quantized_model = quantize_dynamic(\n",
        "#     model,\n",
        "#     {torch.nn.Linear},\n",
        "#     dtype=torch.qint8\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7GzYnYXvQEr"
      },
      "source": [
        "Saving Quantized Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaDBeWDZvRmJ"
      },
      "outputs": [],
      "source": [
        "# torch.save(quantized_model.state_dict(), \"/content/drive/MyDrive/NLP Asg 2/PTQ/bert_ptq.pth\")\n",
        "# import os\n",
        "# print(\"PTQ Model Size (MB):\", os.path.getsize(\"/content/drive/MyDrive/NLP Asg 2/PTQ/bert_ptq.pth\") / 1e6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93KMGUY4vVOS"
      },
      "source": [
        "Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWYJeaTYvV4y"
      },
      "outputs": [],
      "source": [
        "# from datasets import load_dataset\n",
        "# from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "# ds = load_dataset(\"dair-ai/emotion\", \"split\")\n",
        "\n",
        "# test_texts = ds[\"test\"][\"text\"]\n",
        "# test_labels = ds[\"test\"][\"label\"]\n",
        "\n",
        "# preds = []\n",
        "# for text in test_texts:\n",
        "#     inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
        "#     with torch.no_grad():\n",
        "#         logits = quantized_model(**inputs).logits\n",
        "#         preds.append(logits.argmax(dim=-1).item())\n",
        "\n",
        "# macro_f1 = f1_score(test_labels, preds, average=\"macro\")\n",
        "# acc = accuracy_score(test_labels, preds)\n",
        "\n",
        "# print(\"PTQ Macro F1:\", macro_f1)\n",
        "# print(\"PTQ Accuracy:\", acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKFXMe6pvbKw"
      },
      "source": [
        "Measuring Latency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIUf1UGGvcIF"
      },
      "outputs": [],
      "source": [
        "# import time\n",
        "\n",
        "# inputs = tokenizer(\"hello world\", return_tensors=\"pt\")\n",
        "\n",
        "# start = time.time()\n",
        "# for _ in range(100):\n",
        "#     quantized_model(**inputs)\n",
        "# end = time.time()\n",
        "\n",
        "# print(\"Latency (ms per inference):\", (end - start)/100 * 1000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlQqL-CSvdxH"
      },
      "source": [
        "Confusion Matrix + Per-Class F1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzEXjOOBvgf4"
      },
      "outputs": [],
      "source": [
        "# from sklearn.metrics import classification_report, confusion_matrix\n",
        "# import numpy as np\n",
        "\n",
        "# print(classification_report(test_labels, preds, digits=4))\n",
        "# print(confusion_matrix(test_labels, preds))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqDF0YWr2SxU"
      },
      "source": [
        "## 3. QAT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9FBpWMP6QVX"
      },
      "source": [
        "Load Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJvQUZpE2ImK"
      },
      "outputs": [],
      "source": [
        "# from transformers import BertForSequenceClassification, BertTokenizer\n",
        "\n",
        "# model = BertForSequenceClassification.from_pretrained(\n",
        "#     \"/content/drive/MyDrive/NLP Asg 2/bert-emotion-finetuned\",\n",
        "#     local_files_only=True\n",
        "# )\n",
        "\n",
        "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d9DGOU06S05"
      },
      "source": [
        "Freeze Lower Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJ5pvSUy6VkR"
      },
      "outputs": [],
      "source": [
        "# for name, param in model.bert.named_parameters():\n",
        "#     if \"layer.\" in name:\n",
        "#         layer_num = int(name.split(\"layer.\")[1].split(\".\")[0])\n",
        "#         if layer_num < 9:\n",
        "#             param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5S0g3wT6XO3"
      },
      "source": [
        "Prepare Model for QAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2N0F2_i-6Yd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e12f237b-08fa-4825-95ac-503c2782957d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-512020244.py:17: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_prepared = tq.prepare_qat(model)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# import torch\n",
        "# import torch.ao.quantization as tq\n",
        "# model.train()\n",
        "\n",
        "# qat_config = tq.get_default_qat_qconfig(\"fbgemm\")\n",
        "# def apply_qconfig_to_linear_only(module):\n",
        "#     for name, child in module.named_children():\n",
        "#         if isinstance(child, torch.nn.Linear):\n",
        "#             child.qconfig = qat_config\n",
        "#         else:\n",
        "#             child.qconfig = None\n",
        "#         apply_qconfig_to_linear_only(child)\n",
        "\n",
        "# apply_qconfig_to_linear_only(model)\n",
        "\n",
        "\n",
        "# model_prepared = tq.prepare_qat(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHdT_wGj6agr"
      },
      "source": [
        "Fine-Tune for QAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjKItoHy6cq7"
      },
      "outputs": [],
      "source": [
        "# from datasets import load_dataset\n",
        "# from torch.utils.data import DataLoader\n",
        "\n",
        "# ds = load_dataset(\"dair-ai/emotion\", \"split\")\n",
        "\n",
        "# def encode(batch):\n",
        "#     return tokenizer(batch[\"text\"],\n",
        "#                      truncation=True,\n",
        "#                      padding=\"max_length\",\n",
        "#                      max_length=128)\n",
        "\n",
        "# encoded = ds.map(encode, batched=True)\n",
        "# encoded.set_format(\"torch\", columns=[\"input_ids\",\"attention_mask\",\"label\"])\n",
        "\n",
        "# train_loader = DataLoader(encoded[\"train\"], batch_size=16, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer = torch.optim.AdamW(model_prepared.parameters(), lr=1e-5)\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model_prepared.to(device)\n",
        "\n",
        "# for epoch in range(2):\n",
        "#     total_loss = 0\n",
        "#     for batch in train_loader:\n",
        "#         batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "#         outputs = model_prepared(\n",
        "#             input_ids=batch[\"input_ids\"],\n",
        "#             attention_mask=batch[\"attention_mask\"],\n",
        "#             labels=batch[\"label\"]\n",
        "#         )\n",
        "\n",
        "#         loss = outputs.loss\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         total_loss += loss.item()\n",
        "\n",
        "#     print(\"Epoch:\", epoch, \"Loss:\", total_loss)\n"
      ],
      "metadata": {
        "id": "zTgklswpbH1X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebf1f05f-3990-4f1b-d6aa-3475e5d34571"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Loss: 178.73156312759966\n",
            "Epoch: 1 Loss: 130.9577564052306\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOxWEiMJ6eu8"
      },
      "source": [
        "Convert QAT model to int8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5g7CK6e6fMd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c824b8a5-3134-4dc2-f52d-d3f6ceb850b8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/NLP Asg 2/QAT/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/NLP Asg 2/QAT/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/NLP Asg 2/QAT/vocab.txt',\n",
              " '/content/drive/MyDrive/NLP Asg 2/QAT/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# After training QAT\n",
        "# model_prepared.cpu()\n",
        "# model_prepared.eval()\n",
        "\n",
        "# save_dir = \"/content/drive/MyDrive/NLP Asg 2/QAT\"\n",
        "# model_prepared.save_pretrained(save_dir)\n",
        "# tokenizer.save_pretrained(save_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. QLORA"
      ],
      "metadata": {
        "id": "-ozHfSw3TvBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q -U \"bitsandbytes\" \"peft\" \"accelerate\" \"evaluate\""
      ],
      "metadata": {
        "id": "kXCnkA-pPL8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataset and tokenizer"
      ],
      "metadata": {
        "id": "QpuHX6P4cMQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from datasets import load_dataset\n",
        "# from transformers import AutoTokenizer\n",
        "\n",
        "# dataset = load_dataset(\"dair-ai/emotion\", \"split\")\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# MAX_LEN = 128\n",
        "\n",
        "# def tokenize_fn(batch):\n",
        "#     return tokenizer(\n",
        "#         batch[\"text\"],\n",
        "#         truncation=True,\n",
        "#         padding=\"max_length\",\n",
        "#         max_length=MAX_LEN,\n",
        "#     )\n",
        "\n",
        "# tokenized = dataset.map(tokenize_fn, batched=True)\n",
        "# tokenized = tokenized.rename_column(\"label\", \"labels\")\n",
        "# tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
      ],
      "metadata": {
        "id": "SEtafdn2cMFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT in 4-bit and wrap with QLoRA"
      ],
      "metadata": {
        "id": "kx9Ld17td25m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from transformers import AutoModelForSequenceClassification\n",
        "# from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# num_labels = len(set(dataset[\"train\"][\"label\"]))\n",
        "\n",
        "# base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "#     \"bert-base-uncased\",\n",
        "#     num_labels=num_labels,\n",
        "#     load_in_4bit=True,          # 4-bit base weights\n",
        "#     device_map=\"auto\",\n",
        "# )\n",
        "\n",
        "# lora_config = LoraConfig(\n",
        "#     r=8,                        # rank\n",
        "#     lora_alpha=16,\n",
        "#     lora_dropout=0.1,\n",
        "#     bias=\"none\",\n",
        "#     task_type=\"SEQ_CLS\",\n",
        "# )\n",
        "\n",
        "# model = get_peft_model(base_model, lora_config)\n",
        "# model.print_trainable_parameters()\n",
        "\n"
      ],
      "metadata": {
        "id": "27r0Smhsd3dP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trainer (QLoRA fine-tuning)"
      ],
      "metadata": {
        "id": "Q4mMeQfJmmhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import TrainingArguments, Trainer\n",
        "# import evaluate\n",
        "# import numpy as np\n",
        "\n",
        "# accuracy = evaluate.load(\"accuracy\")\n",
        "# f1 = evaluate.load(\"f1\")\n",
        "\n",
        "# def compute_metrics(p):\n",
        "#     preds = np.argmax(p.predictions, axis=1)\n",
        "#     acc = accuracy.compute(predictions=preds, references=p.label_ids)\n",
        "#     f1_macro = f1.compute(predictions=preds, references=p.label_ids, average=\"macro\")\n",
        "#     return {\"accuracy\": acc[\"accuracy\"], \"macro_f1\": f1_macro[\"f1\"]}\n",
        "\n",
        "# # training_args = TrainingArguments(\n",
        "# #     output_dir=\"/content/drive/MyDrive/NLP Asg 2/QLORA/\",\n",
        "# #     report_to=\"none\",\n",
        "# #     eval_strategy=\"epoch\",\n",
        "# #     save_strategy=\"epoch\",\n",
        "# #     learning_rate=2e-4,\n",
        "# #     per_device_train_batch_size=16,\n",
        "# #     per_device_eval_batch_size=16,\n",
        "# #     num_train_epochs=2,\n",
        "# #     weight_decay=0.01,\n",
        "# #     load_best_model_at_end=True,\n",
        "# #     logging_steps=50,\n",
        "# # )\n",
        "\n",
        "# # trainer = Trainer(\n",
        "# #     model=model,\n",
        "# #     args=training_args,\n",
        "# #     train_dataset=tokenized[\"train\"],\n",
        "# #     eval_dataset=tokenized[\"validation\"],\n",
        "# #     compute_metrics=compute_metrics,\n",
        "# # )\n",
        "\n",
        "# # trainer.train()\n"
      ],
      "metadata": {
        "id": "dG35AzU7mmOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save"
      ],
      "metadata": {
        "id": "F6Kry00fmqSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save_dir = \"/content/drive/MyDrive/NLP Asg 2/QLORA FINAL/\"\n",
        "\n",
        "# # model.save_pretrained(save_dir)         # saves only LoRA adapter weights\n",
        "# # tokenizer.save_pretrained(save_dir)"
      ],
      "metadata": {
        "id": "J2eNm_KOo1wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "QLoRA model for evaluation on test set"
      ],
      "metadata": {
        "id": "3x4onHipmxGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save_dir = \"/content/drive/MyDrive/NLP Asg 2/QLORA FINAL/\"\n",
        "# from peft import PeftModel\n",
        "\n",
        "# base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "#     \"bert-base-uncased\",\n",
        "#     num_labels=num_labels,\n",
        "#     load_in_4bit=True,\n",
        "#     device_map=\"auto\",\n",
        "# )\n",
        "\n",
        "# qlora_model = PeftModel.from_pretrained(base_model, save_dir)\n",
        "# qlora_model.eval()\n"
      ],
      "metadata": {
        "id": "eFzOvW-8myxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluate"
      ],
      "metadata": {
        "id": "YMFv8L1Vm08t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch.utils.data import DataLoader\n",
        "# from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# test_loader = DataLoader(tokenized[\"test\"], batch_size=32)\n",
        "\n",
        "# all_preds, all_labels = [], []\n",
        "\n",
        "# for batch in test_loader:\n",
        "#     with torch.no_grad():\n",
        "#         outputs = qlora_model(\n",
        "#             input_ids=batch[\"input_ids\"].to(qlora_model.device),\n",
        "#             attention_mask=batch[\"attention_mask\"].to(qlora_model.device),\n",
        "#         )\n",
        "#     preds = outputs.logits.argmax(dim=-1).cpu().numpy()\n",
        "#     labels = batch[\"labels\"].numpy()\n",
        "#     all_preds.extend(preds.tolist())\n",
        "#     all_labels.extend(labels.tolist())\n",
        "\n",
        "# print(classification_report(all_labels, all_preds, digits=4))\n",
        "# print(confusion_matrix(all_labels, all_preds))\n"
      ],
      "metadata": {
        "id": "sAUG7aNrm1oK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "latency"
      ],
      "metadata": {
        "id": "fnU9vvpem8R7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import time\n",
        "\n",
        "# example = tokenized[\"test\"][0]\n",
        "# inputs = {\n",
        "#     \"input_ids\": example[\"input_ids\"].unsqueeze(0).to(qlora_model.device),\n",
        "#     \"attention_mask\": example[\"attention_mask\"].unsqueeze(0).to(qlora_model.device),\n",
        "# }\n",
        "\n",
        "# N = 100\n",
        "# start = time.time()\n",
        "# with torch.no_grad():\n",
        "#     for _ in range(N):\n",
        "#         _ = qlora_model(**inputs)\n",
        "# end = time.time()\n",
        "# print(\"QLoRA latency (ms/example):\", (end - start) / N * 1000)\n"
      ],
      "metadata": {
        "id": "-kTbiOmIm92z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "GS_Ymhx2yYlg"
      ],
      "authorship_tag": "ABX9TyP2kkckjf0muuC/T2qmRdO0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}